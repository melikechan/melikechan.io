---
title: "Machine Learning Series: How Does the Machine \"Learn\"?"
date: "2024-10-27"
author: "Melike Vurucu"
tags: ["AI", "Artifical Intelligence", "Machine Learning"]
---

(to be edited later)

Well, you may heard of some _machine learning_ memes such as:

<figure style={{textAlign: "center"}}>
    <Image src="/images/ai-series/machine-learning/how-does-the-machine-learn/machine-is-still-learning.png" 
    alt="Machine is still learning" 
    width={0} 
    height={0} 
    sizes="50vw" 
    style={{width: "30%", height: "auto"}} />
    <figcaption style={{margin: 0}}>
    _Well, I am not actually a **fan** of this meme, I saw on LinkedIn (then found on Reddit)... Here is the [Reddit post (original one seems deleted)](https://www.reddit.com/r/ProgrammerHumor/comments/lyejqp/machine_learning_things/)._
    </figcaption>
</figure>

## What actually learning is?!

Well, when we look at the **anatomy of the machine learning methods**, we see that:
> There is a **set of parameters** that should be selected in a way that the **model can make the best predictions**.

Some of the **parameters** are **learned** from the data, and some of them are **predefined**.

### Well, there is a predefined set of parameters, why do we need to learn from the data?

Because they are parameters predefined to _adjust the model's_ behavior, not predicting the output.

Some of the parameters that are **predefined** are:
- **Learning Rate**
- **Number of Iterations/Epochs**
- **Number of Layers**

These parameters are _set before the training_ and _not learned from the data_, are known as **hyperparameters**.

### But, what about the parameters that are learned from the data?

These parameters are learned from the data and usually referred as **weights**.

Some of the parameters that are **learned** from the data are:
- **Weights of the Neurons**
- **Biases of the Neurons**
- **Coefficients of the Features**

These parameters are learned from the data and are used to make the predictions.

> Conclusively:
> - **Hyperparameters** are predefined parameters that are set before the training.
> - **Weights** are learned from the data and are used to make the predictions.

### We learned what is "learned", but how does the machine learn?

Well, by using the **magical power of the optimization algorithms**!

These optimization algorithms are used to **minimize the error** between the **predicted output** and the **actual output**.

#### What is the metric of error?

It is _how our output is different_ from the _actual output!_ 

This metric of error is calculated by using a **loss function**.

> Well, this is another topic to discuss, but in short, **loss function** is a function that calculates the **difference between the predicted output and the actual output**.

Our goal is to _minimize the loss function_ to make the model **learn better**.

#### How do we minimize the loss function?

By using _gradient descent!_

Gradient descent is an optimization algorithm that is used to **minimize the loss function**.

> Well, this is also another topic to discuss, but in short, it is an algorithm that **iteratively updates the weights** to **minimize the loss function**.

## Types of Feeding Data to the Model

We feed data to make the model learn. And, it doesn't have a single way!

There are mainly two types of feeding data to the model:

1. **Batch Learning**: **_Whole dataset_** is fed to the model at once.
    - **Advantages**: Model is stable and as the model sees the whole dataset, it can make better predictions.
    - **Disadvantages**: It is **computationally expensive** and **time-consuming**. Also, the model needs to be **retrained from scratch** if new data is added.
2. **Online Learning**: **Data is fed in _small batches (in mini-batches, a subset of dataset or one by one)_** to the model.
    - **Advantages**: It is **computationally less expensive** and **time-efficient**. Also, the model can be **updated with new data**.
    - **Disadvantages**: Model can be **less stable** and **can forget the previous data**.
